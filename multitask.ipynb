{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8t/rsjbk7k54gv3r11q4h909sx40000gn/T/ipykernel_17840/2562328410.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMultiTaskDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Dataset class\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, df, tfms, size=64):\n",
    "        self.paths = list(df.name)\n",
    "        self.labels = list(df.label)\n",
    "        self.tfms = tfms\n",
    "        self.size = size\n",
    "        self.norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #image net\n",
    "        \n",
    "    def __len__(self): return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx): # using FastAIâ€™s data augmentations instead of torchvision.transform\n",
    "        # dealing with the image\n",
    "        img = PIL.Image.open(self.paths[idx]).convert('RGB')\n",
    "        img = Image(pil2tensor(img, dtype=np.float32).div_(255))\n",
    "        img = img.apply_tfms(self.tfms, size = self.size)\n",
    "        img = self.norm(img.data)\n",
    "        \n",
    "        # dealing with the labels\n",
    "        labels = self.labels[idx].split(\" \")\n",
    "        age = torch.tensor(float(labels[0]), dtype=torch.float32)\n",
    "        gender = torch.tensor(int(labels[1]), dtype=torch.int64)\n",
    "        ethnicity = torch.tensor(int(labels), dtype=torch.int64)\n",
    "        \n",
    "        return img.data, (age.log_()/4.75, gender, ethnicity)  # Taking logs means that errors in predicting high ages and low ages will affect the result equally\n",
    "        # divide by 4.75 as max value of log(age) => results should be between 0 and 1 and MSE shouldn't be bigger than other losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8t/rsjbk7k54gv3r11q4h909sx40000gn/T/ipykernel_17840/617451890.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creating dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtfms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiTaskDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalid_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiTaskDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_transforms' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating dataloader\n",
    "tfms = get_transforms()  #?\n",
    "train_ds = MultiTaskDataset(df_train, tfms[0], size=64)\n",
    "valid_ds = MultiTaskDataset(df_valid, tfms[1], size=64)\n",
    "train_dl = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers = 2)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=128, shuffle=False, num_workers=2)\n",
    "data = DataBunch(train_dl, valid_dl)   #?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "class MultiTaskModel(nn.Module):\n",
    "    \"\"\"_summary_\n",
    "    Create a MTL model with the encoder from \"arch\" and with dropout multiplier ps.\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, arch, ps=0.5):   # arch ? How to input diff target for age, gender, ethnic?\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.endcoder = create_body(arch)   # FastAI function that creates an encoder given an architechture \n",
    "        self.fc1 = create_head(1024, 1, ps=ps) # FastAI function that creates a head ?\n",
    "        self.fc2 = create_head(1024, 2, ps=ps)\n",
    "        self.fc3 = create_head(1024, 5, ps = ps)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.endcoder(x)\n",
    "        age = torch.sigmoid(self.fc1(x))  # take sigmoid to force model to always output a prediction in the acceptable range\n",
    "        gender = self.fc2(x)\n",
    "        ethnicity = self.fc3(x)\n",
    "        \n",
    "        return [age, gender, ethnicity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining loss functions\n",
    "# Letting the model learn how to weight th task specific losses\n",
    "class MultiTaskLossWrapper(nn.Module):\n",
    "    def __init__(self ,task_num):\n",
    "        super(MultiTaskLossWrapper, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        self.log_vars  = nn.Parameter(torch.zeros((task_num)))\n",
    "        \n",
    "    def forward(self, preds, age, gender, ethnicity):\n",
    "        mse, crossEntropy = MSELossFlat(), CrossEntropyFlat() #?\n",
    "        \n",
    "        loss0 = mse(preds[0], age)\n",
    "        loss1 = crossEntropy(preds[1], gender)\n",
    "        loss2 = crossEntropy(preds[2], ethnicity)\n",
    "        \n",
    "        precision0 = torch.exp(-self.log_vars[0])\n",
    "        loss0 = precision0 + loss0 + self.log_vars[0]\n",
    "        \n",
    "        precision1 = torch.exp(-self.log_vars[1])\n",
    "        loss1 = precision1 + loss1 + self.log_vars[1]\n",
    "        \n",
    "        precision2 = torch.exp(-self.log_vars[2])\n",
    "        loss2 = precision2 + loss2 + self.log_vars[2]\n",
    "        \n",
    "        return loss0 + loss1 + loss2\n",
    "    \n",
    "# A principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the learner and training\n",
    "def rmse_age(preods, age, gender, ethnicity): return root_mean_square_error(preds[0], age)\n",
    "def acc_gender(preds, age, gender, ethnicity): return accuracy(preds[1], gender)\n",
    "def acc_ethnicity(preds, age, gender, ethnicity): return accuracy(preds[2], ethnicity)\n",
    "metrics = [rmse_age, acc_gender, acc_ethnicity]\n",
    "\n",
    "model = MultiTaskModel(models.resnet34, ps=0.25)\n",
    "\n",
    "loss_func = MultiTaskLossWrapper(3).to(data.device) # making sure the loss in on the gpu\n",
    "\n",
    "learn = Learner(data,model, loss_func = loss_func, callback_fns = ShowGraph, metrics= metrics)  #? learner\n",
    "\n",
    "# splitting the model so that can use discriminative learning rates\n",
    "learn.split([learn.model.encoder[:6],\n",
    "             learn.model.encoder[6:],\n",
    "             nn.ModuleList([learn.model.fc1, learn.model.fc2, learn.model.fc3])]);\n",
    "\n",
    "# first train only the last layer group (the heads)\n",
    "learn.freeze()\n",
    "\n",
    "\n",
    "# After that unfreeze the encoder and train the whole model with discriminative learning rates for 100 epochs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e2b3cd2829f09aa4cbce26e91031c8f0c76a211d73914c4bed7f6ce2202e3c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
